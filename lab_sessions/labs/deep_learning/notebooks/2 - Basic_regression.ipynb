{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>ARTIFICIAL INTELLIGENCE (E016350A)</b> <br>\n",
    "ALEKSANDRA PIZURICA <br>\n",
    "GHENT UNIVERSITY <br>\n",
    "AY 2024/2025 <br>\n",
    "Assistant: Nicolas Vercheval\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIdT9iu_Z4Rb"
   },
   "source": [
    "# Basic regression: Predict fuel efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AHp3M9ZmrIxj"
   },
   "source": [
    "*Regression* and *Classification* algorithms are supervised learning algorithms. Both algorithms are used for prediction and work with the labelled datasets. The main difference is that regression algorithms predict continuous values such as price, salary, age, etc. Regression is thus a process of finding the correlations between dependent and independent variables. Classification algorithms, on the other hand, predict discrete values such as Male or Female, True or False, Spam or Not Spam, etc. \n",
    "This notebook uses the classic [Auto MPG Dataset](https://archive.ics.uci.edu/ml/datasets/auto+mpg) and builds a model to predict the fuel efficiency of the late-1970s and early 1980s automobiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rRo8oNqZ-Rj"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9xQKvCJ85kCQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F_72b0LCNbjx"
   },
   "source": [
    "\n",
    "## The Auto MPG dataset\n",
    "The dataset is available from the [UCI Machine Learning](https://archive.ics.uci.edu/ml/) repository.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gFh9ne3FZ-On"
   },
   "source": [
    "### Getting the data\n",
    "\n",
    "First download the dataset by using the `keras.utils.get_file` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9kxxgzvzlyz"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import StringIO\n",
    "# We download the data using the request module\n",
    "request = requests.get(\"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\")\n",
    "# We convert the download into a file containing a string with StringIO\n",
    "if request.status_code == 200:  # downloaded without errors\n",
    "    file_str_io = StringIO(request.text)\n",
    "else:\n",
    "    print(\"Download file manually and replace file_srt_io with its path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nslsRLh7Zss4"
   },
   "source": [
    "We will use the `pandas` library to work with the tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CiX2FI4gZtTt"
   },
   "outputs": [],
   "source": [
    "column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "raw_dataset = pd.read_csv(file_str_io, names=column_names,\n",
    "                      na_values = \"?\", comment='\\t',\n",
    "                      sep=\" \", skipinitialspace=True)\n",
    "\n",
    "dataset = raw_dataset.copy()\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3MWuJTKEDM-f"
   },
   "source": [
    "### Clean the data\n",
    "\n",
    "The dataset contains a few unknown values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JEJHhN65a2VV"
   },
   "outputs": [],
   "source": [
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9UPN0KBHa_WI"
   },
   "source": [
    "There are different ways to solve the problem of missing values (their approximation by average or replacement by other values, etc). In our case, we will throw out the instances that have the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ZUDosChC1UN"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Sx_uuefLRHw"
   },
   "source": [
    "### Encoding categorical variables\n",
    "\n",
    "Column `Origin` is a categorical variable and not numerical, as it contains the name of the location the car is coming from:\n",
    "1. (USA)\n",
    "2. (Europe)\n",
    "3. (Japan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lTrlJnzWLHn2"
   },
   "outputs": [],
   "source": [
    "# List unique values in a column (in 2 different ways)\n",
    "print(dataset.Origin.unique())\n",
    "print(dataset[\"Origin\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8XKitwaH4v8h"
   },
   "source": [
    "**TIP:** This is an extremely useful transformation in practice. We want to replace the column elements with other values depending on the mapping we pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gWNTD2QjBWFJ"
   },
   "outputs": [],
   "source": [
    "dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X2bGpaH5MKDY"
   },
   "source": [
    "We perform dummy encoding of a categorical variable. This categorical data encoding method transforms the categorical variable into a set of binary variables (also known as dummy variables). In the case of one-hot encoding, for $N$ categories in a variable, it uses $N$ binary variables. We convert the `Origin` variable to a one-hot vector with `pd.get_dummies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ulXz4J7PAUzk"
   },
   "outputs": [],
   "source": [
    "dataset = pd.get_dummies(dataset, prefix='', prefix_sep='', dtype=int)\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cuym4yvk76vU"
   },
   "source": [
    "### Split the data into training and test sets\n",
    "\n",
    "Now split the dataset into a training set and a test set.\n",
    "\n",
    "Use the test set in the final evaluation of your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qn-IGhUE7_1H"
   },
   "outputs": [],
   "source": [
    "# We choose 80% of the data as the training data\n",
    "train_dataset = dataset.sample(frac=0.8,random_state=0)\n",
    "\n",
    "# The rest 20% is used as the testing data\n",
    "test_dataset = dataset.drop(train_dataset.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J4ubs136WLNp"
   },
   "source": [
    "### Inspect the data\n",
    "\n",
    "Have a quick look at the joint distribution of a few pairs of columns from the training set.\n",
    "\n",
    "Looking at the top row it should be clear that the fuel efficiency (MPG) is a function of all the other parameters. Looking at the other rows it should be clear that they are functions of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oRKO_x8gWKv-"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(train_dataset[[\"MPG\", \"Cylinders\", \"Displacement\", \"Weight\"]], diag_kind=\"kde\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gavKO_6DWRMP"
   },
   "source": [
    "Also look at the overall statistics, note how each feature covers a different range. We omit the MPG as this is the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yi2FzC3T21jR"
   },
   "outputs": [],
   "source": [
    "train_stats = train_dataset.astype(float).describe()\n",
    "train_stats.pop(\"MPG\")\n",
    "train_stats = train_stats.transpose()\n",
    "train_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Db7Auq1yXUvh"
   },
   "source": [
    "### Extract the target variable\n",
    "\n",
    "Separate the target value, the \"label\", from the features. This label is the value that you will train the model to predict.\n",
    "\n",
    "So, we extract the target variable `MPG` from the data ([miles per gallon](https://www.carwow.co.uk/guides/running/what-is-mpg-0255))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t2sluJdCW7jN"
   },
   "outputs": [],
   "source": [
    "train_labels = train_dataset.pop(\"MPG\")\n",
    "test_labels = test_dataset.pop(\"MPG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRklxK5s388r"
   },
   "source": [
    "### Data standardization/normalization\n",
    "\n",
    "It is good practice to normalize features that use different scales and ranges.\n",
    "\n",
    "One reason this is important is because the features are multiplied by the model weights. So the scale of the outputs and the scale of the gradients are affected by the scale of the inputs.\n",
    "\n",
    "Although a model might converge without feature normalization, normalization makes training much more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JlC5ooJrgjQF"
   },
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "  return (x.astype(float) - train_stats['mean']) / train_stats['std']\n",
    "normed_train_data = norm(train_dataset)\n",
    "normed_test_data = norm(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ywmerQ6dSox"
   },
   "source": [
    "**IMPORTANT:** Note that we use the average and standard deviation of the training set both when we standardize data for training **and** testing. This is important because we do not want to use the information from the test data set in any way when training the model because it introduces a bias that leads to customization. Check the notebooks from the previous Lab session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SmjdzxKzEu1-"
   },
   "source": [
    "## Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6SWtkIjhrZwa"
   },
   "source": [
    "### Defining the model\n",
    "\n",
    "It is time to define our model. We will use the `Sequential` model, representing one neural network with forward propagation. At the output of this network, there will be a neuron that will evaluate the attribute `MPG`.\n",
    "\n",
    "We select the mean squared error as the loss function.\n",
    "\n",
    "Apart from the `Adam` optimizer, there are many others. For illustration, the `RMSprop` optimizer will be used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c26juK7ZG8j-"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "  model = keras.Sequential([\n",
    "    layers.Input(shape=(len(train_dataset.keys()), )),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "  model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae', 'mse'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cGbPb-PHGbhs"
   },
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sj49Og4YGULr"
   },
   "source": [
    "### Model summary\n",
    "\n",
    "Using the `summary` function we can look at an overview of the defined model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ReAD0n6MsFK-"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vt6W50qGsJAL"
   },
   "source": [
    "We can test the model. We will take a subset of $10$ examples from the training set and pass them through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-d-gBaVtGTSC"
   },
   "outputs": [],
   "source": [
    "example_batch = normed_train_data[:10]\n",
    "example_result = model.predict(example_batch)\n",
    "example_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "MLJ4i_UyeY0f",
    "outputId": "ca4e6e86-1d2c-4f5e-ad12-eacdea248f94"
   },
   "outputs": [],
   "source": [
    "example_result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QlM8KrSOsaYo"
   },
   "source": [
    "We get $10$ values as predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0-qWCsh6DlyH"
   },
   "source": [
    "### Training the model\n",
    "\n",
    "We divide the training set into two new sets. With the first (80% of the original training set) optimizes its parameters through backpropagation, with the second validation we can evaluate its performance and tune the hyperparameters.\n",
    "\n",
    "We will train the 100 epoch model and keep the accuracy of the training and validation data during the training. The `fit` function returns an object that contains the necessary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sD7qHCmNIOY0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "\n",
    "history = model.fit(\n",
    "  normed_train_data, train_labels,\n",
    "  epochs=EPOCHS, validation_split = 0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "RGZCiXyyXYJa",
    "outputId": "c5f0086c-695b-42cb-d22f-76d2cfe22276"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.epoch, history.history['mse'])\n",
    "plt.plot(history.epoch, history.history['val_mse'])\n",
    "plt.legend(['Training MSE', 'Validation MSE'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQm3pc0FYPQB"
   },
   "source": [
    "The obtained data can be visualized using the `pandas` library.\n",
    "`DataFrame` is a `pandas` type that represents tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Xj91b-dymEy"
   },
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "\n",
    "# We take the last 5 rows\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AqsuANc11FYv"
   },
   "source": [
    "We can notice that 200 epochs are too much and that releasing training to last that long does not contribute to the model's accuracy. We will repeat the optimization process again, but this time, we will use a technique called *early stopping*.\n",
    "\n",
    "The idea is to define a set of constraints that, once fulfilled, model training will be stopped. For example, if the value of `val_mse` does not improve in `k` consecutive epochs, it makes sense to stop training.\n",
    "\n",
    "How do we determine the `k` parameter? It is a hyperparameter as the network architecture, optimizer, etc.\n",
    "\n",
    "More about `EarlyStopping` can be found [here](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fdMZuhUgzMZ4"
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "\n",
    "# Parameter `patience` is the number of epochs considered for the early stopping.\n",
    "# Parameter `monitor` represents the measure being compared through the epoch.\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "early_history = model.fit(normed_train_data, train_labels, \n",
    "                    epochs=EPOCHS, validation_split=0.2, verbose=1, \n",
    "                    callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "id": "y8C67DpmZjIU",
    "outputId": "7cddf725-78f9-4ab0-bb95-a4e45cd69411"
   },
   "outputs": [],
   "source": [
    "plt.plot(early_history.epoch, early_history.history['mae'])\n",
    "plt.plot(early_history.epoch, early_history.history['val_mae'])\n",
    "plt.ylim([0, 10])\n",
    "plt.legend(['Training MAE', 'Validation MAE'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE [MPG]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3St8-DmrX8P4"
   },
   "source": [
    "The graph shows that at the validation set, the average error is about $\\pm 2$ MPG. Whether this is good or not depends on the measure and the case of use, which depends on the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OKvwopHue1dW"
   },
   "source": [
    "### Model evaluation\n",
    "\n",
    "Next, we will look at how well the model generalizes on the test data.\n",
    "\n",
    "So far, we have trained the model in a subset (80%) of the training set to have a validation set. There is no point in throwing away data (20% of training data that ended up as validation data), so we will re-train the model with the entire training data for the test evaluation.\n",
    "\n",
    "How many epochs should we train it for? One approach is to set the number of epochs of the early stop. Let us put 90."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Mu-Bbbi-cIOR",
    "outputId": "3b736b32-6379-4c7d-a7f8-bb92044b511a"
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "\n",
    "early_stop_epochs = 90\n",
    "\n",
    "final_history = model.fit(normed_train_data, train_labels, \n",
    "                    epochs=early_stop_epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "76RVqUtdcovq"
   },
   "source": [
    "Finally, we can look at how our model behaves at the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jl_yNr5n1kms"
   },
   "outputs": [],
   "source": [
    "loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=2)\n",
    "rmse_test = np.sqrt(mse)\n",
    "\n",
    "print(\"Testing set Mean Abs Error: {:5.2f} MPG\".format(mae))\n",
    "print(\"Testing set Root of the Mean Squared Errorr: {:5.2f} MPG\".format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DhFbCnYcswV"
   },
   "source": [
    "The test error should be less than the validation error because we are now using more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ft603OzXuEZC"
   },
   "source": [
    "### Predicting values in the future\n",
    "\n",
    "Is it over? Yes and no, it depends on different things. If we are satisfied with this model and want to move it into production and use it, there is no point in throwing away the data in the test set.\n",
    "\n",
    "It makes sense to retrain the model, now over all the data. How do we then evaluate that model? We will not evaluate it. If we conducted this process well, the error in the test set would approximate the quality of this final model we trained over the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is partially based on the official [Basic regression: Predict fuel efficiency\n",
    "](https://www.tensorflow.org/tutorials/keras/regression) Tensorflow tutorial."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tf.keras.fuel.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
