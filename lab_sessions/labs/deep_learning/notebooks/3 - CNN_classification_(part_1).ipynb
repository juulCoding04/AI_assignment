{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>ARTIFICIAL INTELLIGENCE (E016350A)</b> <br>\n",
    "ALEKSANDRA PIZURICA <br>\n",
    "GHENT UNIVERSITY <br>\n",
    "AY 2024/2025 <br>\n",
    "Assistant: Nicolas Vercheval\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DSPCom-KmApV"
   },
   "source": [
    "# Convolutional neural networks (CNN) - image classification (part 1)\n",
    "\n",
    "\n",
    "This tutorial demonstrates training a simple Convolutional Neural Network (CNN) to classify [CIFAR](https://www.cs.toronto.edu/~kriz/cifar.html) images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7KBpffWzlxH"
   },
   "source": [
    "### Import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iAve6DCL4JH4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jRFxccghyMVo"
   },
   "source": [
    "### CIFAR10 dataset\n",
    "\n",
    "The CIFAR10 dataset contains 60,000 color images in 10 classes, with 6,000 images in each class. The dataset is divided into 50,000 training images and 10,000 testing images. The classes are mutually exclusive and there is no overlap between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JWoEqyMuXFF4"
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-fZiQdMg7tWh"
   },
   "outputs": [],
   "source": [
    "train_images[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7wArwCTJJlUa"
   },
   "source": [
    "To verify that the dataset looks correct, we plot the first 25 images from the training set and display the class name below each image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K3PAELE2eSU9"
   },
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    # The CIFAR labels happen to be arrays, \n",
    "    # which is why you need the extra index\n",
    "    plt.xlabel(class_names[train_labels[i][0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oewp-wYg31t9"
   },
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3hQvqXpNyN3x"
   },
   "source": [
    "Below, we will define the first part of our model. In this section, we will explain the [convolutional](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) and [pooling](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D) layers which aim to learn a new representation of the data. With the new data representation, we will later facilitate the work of the classification model (the second part of our model).\n",
    "\n",
    "Convolutional layers are created at the library level using the `Conv2D` function. The number of filters is specified first (parameter `filters`, usually unnamed), then kernel sizes (`kernel_size` parameter), displacement size (`strides` parameter) and padding (`padding` parameter). <img src = https://upload.wikimedia.org/wikipedia/commons/5/55/Convolution_arithmetic_-_Arbitrary_padding_no_strides.gif style = 'height: 300px'>\n",
    "\n",
    "As input, the grid will take the dimension input (image height, image width, channel number) with an additional dimension used for the batch size. The number of channels will be $3$ because the CIFAR10 images are in colour, i.e., there are red, green and blue channels.\n",
    "\n",
    "For example, the figure shows a 3x3 kernel (grey square) that passes through an input (blue square) with a horizontal and vertical offset of size 2. Adding a white set of squares represents a framing and, depending on its presence, the size of the output image (green square) can be of the exact dimensions (in the Keras library, this is emphasized by the value of the `same` parameter `padding`) or somewhat smaller (in the Keras library this is noted by the value of the `valid` parameter` padding`).\n",
    "\n",
    "The pooling layers (`MaxPooling2D` and` AvgPooling2D`) reduce the layers by reducing the blocks of the given majority to their maximum or average values. The `pool_size` parameter specifies the block size. <img src = https://d3i71xaburhd42.cloudfront.net/65a6f29bb5d9418f7ef6547c612d3c3445b7f962/3-Figure1-1.png style = 'width: 600px'>\n",
    "\n",
    "As the dimensions of the images are $32\\times 32$, the network will take the input size $(32,32,3)$, or $(3,32,32)$ in some other libraries that expect the number of channels to go first.\n",
    "\n",
    "To define the input dimension, we can set the named argument `input_shape` when constructing the first layer in the` Sequential` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L9YmGQBQPrdn"
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "# We add a convolutional layer that has 32 3x3 filters with\n",
    "# relu activation function.\n",
    "model.add(layers.Input(shape=(32, 32, 3)))\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "# We add a pooling layer that uses the maximum function\n",
    "# where the filter size is 2x2.\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lvDVFkg-2DPm"
   },
   "source": [
    "Let's display the architecture of your model so far by using the `summary` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8-C4XBg4UTJy"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_j-AXYeZ2GO5"
   },
   "source": [
    "You can see above that the output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels). The width and height dimensions tend to shrink as you go deeper into the network. The first argument controls the number of output channels for each Conv2D layer (e.g., 32 or 64). Typically, as the width and height shrink, you can afford (computationally) to add more output channels in each Conv2D layer.\n",
    "\n",
    "The formula for calculating the dimensions of the output of convolution and pooling is given by:\n",
    "$$\n",
    "O_w = \\frac{W - K + 2P}{S} + 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "O_h = \\frac{H - K + 2P}{S} + 1,\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $O_w$: input dimension - width\n",
    "- $O_h$: output dimension - height\n",
    "- $W$: width\n",
    "- $H$: height\n",
    "- $P$: padding\n",
    "- $S$: stride\n",
    "\n",
    "Often, width and height are equal ($W = H$). For example, in the first convolutional layer, we added transforms\n",
    "(32, 32, 3) into (30, 30, 32).\n",
    "\n",
    "- $W = H = 32$\n",
    "- $H = 32$\n",
    "- $P = 0$\n",
    "- $S = 1$\n",
    "- $K = 3$\n",
    "\n",
    "$$\n",
    "O = \\frac{32 - 3 + 2 \\cdot 0}{1} + 1 = 32 - 3 + 0 + 1 = 30\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_v8sVOtG37bT"
   },
   "source": [
    "### Classification model\n",
    "\n",
    "The second part of our model will be the classification part and will be similar to the previous exercise, where we made a classification model for predicting fuel efficiency.\n",
    "\n",
    "As the current output of the model gives something of a shape (4, 4, 64), it is necessary to transform it into a vector that will be given as the input to a fully connected neural network that will represent the classifier. We use `Flatten` to transform the images into vectors (the batch dimension stays the same).\n",
    "\n",
    "That is, if a tensor Tensor has dimensions $(\\text{batch size}, 4, 4, 64)$, Flatten(Tensor) will be a tensor of dimensions $(\\text{batch size}, 4\\times 4\\times 64) = (\\text{batch size}, 1024)$.\n",
    "\n",
    "CIFAR has ten output classes, so you use a final Dense layer with ten outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mRs95d6LUVEi"
   },
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ipGiQMcR4Gtq"
   },
   "source": [
    "Here's the complete architecture of your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Yu_m-TZUWGX"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P3odqfHP4M67"
   },
   "source": [
    "### Compiling and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MdDzI75PUXrG"
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "\n",
    "train_labels_cat = tf.keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels_cat = tf.keras.utils.to_categorical(test_labels, num_classes)\n",
    "\n",
    "print(f'train_labels.shape={train_labels.shape}')\n",
    "print(f'test_labels.shape={test_labels.shape}')\n",
    "print(f'train_labels_cat.shape={train_labels_cat.shape}')\n",
    "print(f'test_labels_cat.shape={test_labels_cat.shape}')\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(train_images, train_labels_cat, epochs=epochs,\n",
    "                    batch_size=64,  validation_data=(test_images, test_labels_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jKgyC5K_4O0d"
   },
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gtyDF0MKUcM7"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels_cat, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0LvwaKhtUdOo"
   },
   "outputs": [],
   "source": [
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8cfJ8AR03gT5"
   },
   "source": [
    "Your simple CNN has achieved a test accuracy of around 70%. Of course, this can be way better. \n",
    "In the following notebook example, we will see some of the techniques we can use to improve the accuracy by using image augmentation as well as dropout regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to improve this accuracy yourself! Some of the ideas:\n",
    "- Use a deeper model\n",
    "- Increase the number of filters\n",
    "- Train the network longer\n",
    "- Change `batch_size`\n",
    "- Use a dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is partially based on the official Tensorflow [tutorial](https://www-tensorflow-org.translate.goog/tutorials/images/cnn?_x_tr_sl=en&_x_tr_tl=nl&_x_tr_hl=nl&_x_tr_pto=nui%2Csc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "03.tf.cnn.cifar.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
